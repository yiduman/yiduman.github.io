<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Prompt Tuning | Porco Rosso 写字的地方</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="本文旨在结合原文与Peft源码，介绍Prompt Tuning的以下内容   可训练参数的位置  参数结构  前向传播流程">
<meta property="og:type" content="article">
<meta property="og:title" content="Prompt Tuning">
<meta property="og:url" content="http://example.com/2024/03/08/01_prompt_tuning/index.html">
<meta property="og:site_name" content="Porco Rosso 写字的地方">
<meta property="og:description" content="本文旨在结合原文与Peft源码，介绍Prompt Tuning的以下内容   可训练参数的位置  参数结构  前向传播流程">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="http://example.com/assets/one.png">
<meta property="article:published_time" content="2024-03-08T12:54:13.706Z">
<meta property="article:modified_time" content="2024-03-08T13:06:49.550Z">
<meta property="article:author" content="Porco Rosso">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/assets/one.png">
  
    <link rel="alternate" href="/atom.xml" title="Porco Rosso 写字的地方" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Porco Rosso 写字的地方</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Suche"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-01_prompt_tuning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/08/01_prompt_tuning/" class="article-date">
  <time class="dt-published" datetime="2024-03-08T12:54:13.706Z" itemprop="datePublished">2024-03-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Prompt Tuning
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本文旨在结合原文与Peft源码，介绍Prompt Tuning的以下内容</p>
<ul>
<li><input disabled="" type="checkbox"> 可训练参数的位置</li>
<li><input disabled="" type="checkbox"> 参数结构</li>
<li><input disabled="" type="checkbox"> 前向传播流程</li>
</ul>
<span id="more"></span>

<h2 id="可训练参数的位置"><a href="#可训练参数的位置" class="headerlink" title="可训练参数的位置"></a>可训练参数的位置</h2><p>Prompt Tuning只引入一层参数，位于Transformer的输入位置（图中绿色部分）</p>
<p><img src="/../assets/one.png"></p>
<h2 id="参数结构"><a href="#参数结构" class="headerlink" title="参数结构"></a>参数结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># peft_model.py </span></span><br><span class="line"><span class="comment"># line 389</span></span><br><span class="line"><span class="comment"># named_param = &#x27;word_embeddings.weight&#x27;</span></span><br><span class="line">self.word_embeddings = transformer_backbone.get_submodule(named_param.replace(<span class="string">&quot;.weight&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># peft_model.py </span></span><br><span class="line"><span class="comment"># line 393</span></span><br><span class="line"><span class="comment"># config是PromptTuning的参数</span></span><br><span class="line"><span class="comment"># self.word_embedding是原模型最底层Transformer的word_embedding</span></span><br><span class="line">prompt_encoder = PromptEmbedding(config, self.word_embeddings)</span><br></pre></td></tr></table></figure>

<p>用于推理的参数是prompt_encoder对象</p>
<blockquote>
<p>The parameter cost of our method is EP, where E is the token embedding dimension and P is the prompt length.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PromptEmbedding</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, word_embeddings</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules</span><br><span class="line">        self.embedding = torch.nn.Embedding(total_virtual_tokens, config.token_dim)</span><br><span class="line">        <span class="keyword">if</span> config.prompt_tuning_init == PromptTuningInit.TEXT <span class="keyword">and</span> <span class="keyword">not</span> config.inference_mode:</span><br><span class="line">            <span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">            tokenizer_kwargs = config.tokenizer_kwargs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path, **tokenizer_kwargs)</span><br><span class="line">            init_text = config.prompt_tuning_init_text</span><br><span class="line">            init_token_ids = tokenizer(init_text)[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">            num_text_tokens = <span class="built_in">len</span>(init_token_ids)</span><br><span class="line">            <span class="keyword">if</span> num_text_tokens &gt; total_virtual_tokens:</span><br><span class="line">                init_token_ids = init_token_ids[:total_virtual_tokens]</span><br><span class="line">            <span class="keyword">elif</span> num_text_tokens &lt; total_virtual_tokens:</span><br><span class="line">                num_reps = math.ceil(total_virtual_tokens / num_text_tokens)</span><br><span class="line">                init_token_ids = init_token_ids * num_reps</span><br><span class="line">            init_token_ids = init_token_ids[:total_virtual_tokens]</span><br><span class="line">            init_token_ids = torch.LongTensor(init_token_ids).to(word_embeddings.weight.device)</span><br><span class="line"></span><br><span class="line">            word_embedding_weights = word_embeddings(init_token_ids).detach().clone()</span><br><span class="line">            word_embedding_weights = word_embedding_weights.to(torch.float32)</span><br><span class="line">            self.embedding.weight = torch.nn.Parameter(word_embedding_weights)</span><br></pre></td></tr></table></figure>

<p><code>total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules</code>代表允许prompt tuning插入的tokens长度</p>
<p><code>config.prompt_tuning_init_text</code>是初始化的prompt文本</p>
<p>整段代码的工作逻辑是</p>
<ol>
<li>传入预设的prompt tuning参数以及原大模型底层transformer的embedding层</li>
<li>获取原大模型使用的tokenizer</li>
<li>利用tokenizer将prompt文本转化成token ids，并截取或扩充至<code>total_virtual_tokens</code>长度</li>
<li>使用resize后的token ids从embeding选择初始化参数（peft的选择）</li>
</ol>
<p>以下是prompt tuning关于初始化的原文<strong>（2.1  Design Decisions）</strong>：</p>
<blockquote>
<p>A more sophisticated option is to initialize each prompt token to an embedding drawn from the model’s vocabulary.</p>
<p>Since we want the model to produce these tokens in the output, initializing the prompt with the embeddings of the valid target tokens should prime the model to restrict its output to the legal output classes.</p>
</blockquote>
<h2 id="前向传播流程"><a href="#前向传播流程" class="headerlink" title="前向传播流程"></a>前向传播流程</h2><p>假如word vector的长度是1024，引入Prompt Tuning之前，$input.shape&#x3D;8\times8000\times1024$，引入长度为8的Promt Tuning，得到参数维度 $prompt_encoder.embedding.weight.shape &#x3D; 8 \times 8\times 1024$，拼接成新输入$\hat{input}.shape&#x3D;8 \times 8008\times1024$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># peft_model.py</span></span><br><span class="line"><span class="comment"># line 1123~1130</span></span><br><span class="line">inputs_embeds = self.word_embeddings( input_ids) <span class="comment"># 原始输入</span></span><br><span class="line">prompts = prompt_encoder.embedding.weight.repeat( batch_size, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># prompt tuning 参数</span></span><br><span class="line">inputs_embeds = torch.cat( ( prompts, inputs_embeds), dim=<span class="number">1</span>) <span class="comment"># 组合输入</span></span><br></pre></td></tr></table></figure>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/peft">huggingface&#x2F;peft</a></p>
<p>[The Power of Scale for Parameter-Efficient Prompt Tuning]([<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.08691">2104.08691] The Power of Scale for Parameter-Efficient Prompt Tuning (arxiv.org)</a>)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/08/01_prompt_tuning/" data-id="cltinvvvv0000no9g9kgb82xk" data-title="Prompt Tuning" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/01/16/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">随便写写</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Life/" rel="tag">Life</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 20px;">Algorithm</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/Life/" style="font-size: 10px;">Life</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/08/01_prompt_tuning/">Prompt Tuning</a>
          </li>
        
          <li>
            <a href="/2024/01/16/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/">随便写写</a>
          </li>
        
          <li>
            <a href="/2024/01/16/%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/">Python魔法函数之__new__</a>
          </li>
        
          <li>
            <a href="/2024/01/16/0152/">0152.一道非典型动态规划题</a>
          </li>
        
          <li>
            <a href="/2024/01/03/0689/">0698.深度优先搜索</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Porco Rosso<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>