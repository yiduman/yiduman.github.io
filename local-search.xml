<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Python魔法函数之__new__</title>
    <link href="/2024/03/10/%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/"/>
    <url>/2024/03/10/%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p>文中的演示代码基于CPython&#x2F;Python 3.9</p><span id="more"></span><p>阅读Python 3 <code>collections</code>的<code>namedtuple</code>源码时，被<code>__new__</code>弄得很糊涂，翻看了官方教程和源码，藉此梳理</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>如<em>代码1</em>所示，定义一个<code>User</code>类，并实例化两个对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 代码.1</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">User</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__new__</span>(<span class="hljs-params"> cls, *args, **kwargvs</span>):<br>        <span class="hljs-built_in">print</span>( <span class="hljs-string">f&quot;In <span class="hljs-subst">&#123;cls&#125;</span> new function&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().__new__( cls)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name = <span class="hljs-string">&quot;Juboge&quot;</span></span>):<br>        self.name = name<br>        <span class="hljs-built_in">print</span>( <span class="hljs-string">f&quot;In <span class="hljs-subst">&#123;self&#125;</span> init function, with name = <span class="hljs-subst">&#123;name&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    student = User( <span class="hljs-string">&quot;One&quot;</span>)<br>    <span class="hljs-built_in">print</span>()<br>    student_two = User( <span class="hljs-string">&quot;Two&quot;</span>)<br></code></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">In &lt;class &#x27;__main__.User&#x27;&gt; new function<br>In &lt;__main__.User object at 0x000001CD83CACBE0&gt; init function, with name = One<br><br>In &lt;class &#x27;__main__.User&#x27;&gt; new function<br>In &lt;__main__.User object at 0x000001CD83CACD30&gt; init function, with name = Two<br></code></pre></td></tr></table></figure><p>Python的解释器在执行时，<code>new</code>和<code>init</code>的第一个参数有区别</p><ul><li>遇到<code>new</code>魔法函数时，第一个参数传入的是类本身<code>class</code></li><li>遇到<code>init</code>魔法函数时，第一个参数传入的是类的实例化对象<code>instance</code></li></ul><blockquote><p><code>cls</code>和<code>self</code>的命名是一种约定习惯，并非强制规范，这么命名避免混淆</p></blockquote><p><code>student = User( &quot;One&quot;)</code>语句依次调用了这两个魔法函数，<code>new</code>调用在<code>init</code>之前，<code>new</code>方法用于实例化对象，<code>init</code>方法用于初始化对象。<strong>new方法的作用，是控制类的实例化过程</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">student = User( <span class="hljs-string">&quot;One&quot;</span>)<br><span class="hljs-comment"># 等价于</span><br>student = __new__( User, ( <span class="hljs-string">&quot;One&quot;</span>, ) , &#123;&#125; )<br>__init__( student, ( <span class="hljs-string">&quot;One&quot;</span>, ), &#123;&#125; )<br></code></pre></td></tr></table></figure><h2 id="metaclass关键字"><a href="#metaclass关键字" class="headerlink" title="metaclass关键字"></a>metaclass关键字</h2><p>如<em>代码2</em> 所示，引入<code>metaclass</code>关键字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 代码.2</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">UserMetaClass</span>( <span class="hljs-title class_ inherited__">type</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__new__</span>(<span class="hljs-params"> cls, *args, **kwargs</span>):<br>        <span class="hljs-built_in">print</span>( <span class="hljs-string">f&quot;In <span class="hljs-subst">&#123;cls&#125;</span> new function&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().__new__(  cls, *args, **kwargs )<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NewUser</span>( metaclass = UserMetaClass):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__new__</span>(<span class="hljs-params"> cls, *args, **kwargvs</span>):<br>        <span class="hljs-built_in">print</span>( <span class="hljs-string">f&quot;In <span class="hljs-subst">&#123;cls&#125;</span> new function&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().__new__( cls)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name = <span class="hljs-string">&quot;Juboge&quot;</span></span>):<br>        <span class="hljs-built_in">print</span>( <span class="hljs-string">f&quot;In <span class="hljs-subst">&#123;self&#125;</span> new function&quot;</span>)<br>        self.name = name<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    <span class="hljs-comment"># nstudent = NewUser( &quot;One&quot;)</span><br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">In &lt;class &#x27;__main__.UserMetaClass&#x27;&gt; new function<br></code></pre></td></tr></table></figure><p>在解释 <em>代码2</em> 结果之前，必须先强调一个重要的概念——<strong>一切皆对象</strong>，这个一切不限于：</p><ul><li>函数 function</li><li>模板 module</li><li>变量 variable</li><li>实例 instance</li><li>类 class</li></ul><p>没错，<em><strong>类也是对象</strong></em></p><h3 id="两个类"><a href="#两个类" class="headerlink" title="两个类"></a>两个类</h3><p>python中有两个重要的类：</p><ul><li><code>object</code>：python的始祖类，是继承树的根节点，都由<code>object</code>类派生或者迭代派生而来</li><li><code>type</code>: python的模板类，python中的任何对象，都由<code>type</code>类实例化或者迭代实例化而来</li></ul><p><code>object</code>类和<code>type</code>类他俩啥关系？他们同样适用这两个规则，<code>object</code>是<code>type</code>的实例化对象，<code>object</code>同时又是<code>type</code>的基类。</p><p>因此<code>issubclass( type, object)</code>和<code>isinstance( object, type)</code>这两条语句的返回值都是<code>True</code></p><p>举一个具体一点的例子：</p><p>字符串<code>&quot;abc&quot;</code>它是<code>str</code>类的实例化对象，而<code>str</code>类本身，是<code>type</code>类的实例化对象。因此<code>&quot;abc&quot;.__class__.__class__</code>的值为<code>type</code>，<em>代码1</em>中的<code>User</code>类，它当然也是<code>type</code>类的实例化对象，<code>student.__class__.__class__</code>的值也为<code>type</code></p><p>如图所示，是字符串<code>“abc”</code>，字符串类<code>string</code>、<code>object</code>类和<code>type</code>类之间的关系：</p><p><img src="/../assets/003.svg" alt="插图1"></p><h3 id="metaclass"><a href="#metaclass" class="headerlink" title="metaclass"></a>metaclass</h3><p><em>代码1</em> 中的类定义语句，缺省条件下，默认继承<code>object</code>，元类是<code>type</code></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">class</span> <span class="hljs-symbol">User:</span><br><span class="hljs-symbol">    <span class="hljs-symbol">pass</span></span><br># 由于类的声明没有加其它关键字，等价于：<br><span class="hljs-symbol">class</span> <span class="hljs-symbol">User</span>( <span class="hljs-symbol">object, <span class="hljs-symbol">metaclass</span></span> = <span class="hljs-symbol">type</span>):<br>    <span class="hljs-symbol">pass</span><br></code></pre></td></tr></table></figure><p>回到代码2，<code>Newuser</code>引入了<code>metaclass = UserMetaClass</code>语句，将自己的元类设成了<code>UserMetaClass</code>，也就是说<code>NewUser</code>被视为<code>UserMetaClass</code>的实例化对象。</p><blockquote><p>元类需要继承<code>type</code></p></blockquote><p>在Python解释器视角下，<code>NewUser</code>的定义过程，即是<code>UserMetaClass</code>类实例化的过程。上一章我们说过，<code>new</code>方法用于控制类的实例化过程，因此定义<code>NewUser</code>时，会调用<code>UserMetaClass</code>的<code>new</code>方法</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">NewUser</span>( <span class="hljs-title">metaclass</span> = <span class="hljs-type">UserMetaClass</span>):</span><br><span class="hljs-class"># 等价于</span><br><span class="hljs-class"><span class="hljs-type">NewUser</span> = <span class="hljs-type">UserMetaClass</span>.__new__( <span class="hljs-type">UserMetaCLass</span>, *<span class="hljs-title">args</span>, **<span class="hljs-title">kwargs</span> )</span><br></code></pre></td></tr></table></figure><p>这也是<em>代码2</em> 会打印信息的原因。<em>代码2</em> 执行过程，<code>NewUser</code>类自身的<code>new</code>方法没有被调用过，因为<code>NewUser</code>没有实例化对象。</p><h2 id="尾记"><a href="#尾记" class="headerlink" title="尾记"></a>尾记</h2><p>本篇博客着重强调了两个概念：</p><ol><li><strong>new魔法函数用于控制类的实例化过程</strong></li><li><strong>类也是对象</strong></li></ol><p><em>代码1</em> 中的的<code>new</code>方法执行了两次，因为<code>User</code>类实例化了两次；而<em>代码2</em> 中的<code>new</code>方法只执行了一次，因为<code>UserMetaClass</code>只实例化了一次（定义<code>NewUser</code>类）。很多博客会介绍<code>new</code>方法，如果不强调<strong>类也是对象</strong>，我们很容易将类自身的<code>new</code>方法和元类的<code>new</code>方法混淆。</p><p>笔者将在下一篇博客中探讨：</p><ul><li><input disabled="" type="checkbox"> <code>type</code>类</li><li><input disabled="" type="checkbox"> <code>metaclass</code>的传参过程</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随便写写</title>
    <link href="/2024/03/10/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/"/>
    <url>/2024/03/10/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/</url>
    
    <content type="html"><![CDATA[<p>记录生活</p><span id="more"></span><h4 id="2024年3月11日01-25-43"><a href="#2024年3月11日01-25-43" class="headerlink" title="[2024年3月11日01:25:43]"></a>[2024年3月11日01:25:43]</h4><blockquote><p> 我并不是立意要错过，可是我一直都这样做。——席慕蓉《送别》</p></blockquote><p>唯物主义，也许等到宇宙热寂重新坍缩成质点时会再度相聚</p><p>想到自己只是构成“我”之物质的暂时主人，不禁哑然失笑</p><p>和她恢复了通讯</p><p>她每次出现都能带给我正面、积极的改变，好好加油吧</p><p>坦诚、勇敢一点</p><p>如果可以，我会好好待她的</p><h4 id="2024年3月4日00-08-55"><a href="#2024年3月4日00-08-55" class="headerlink" title="[2024年3月4日00:08:55]"></a>[2024年3月4日00:08:55]</h4><p>最近在读苏轼和王安石的传记，嘉佑朝真有趣，众正盈朝不为过</p><h4 id="2024年1月16日01-04-00"><a href="#2024年1月16日01-04-00" class="headerlink" title="[2024年1月16日01:04:00]"></a>[2024年1月16日01:04:00]</h4><p>我要恢复博客了</p><h4 id="2022年7月08日14-00-00"><a href="#2022年7月08日14-00-00" class="headerlink" title="[2022年7月08日14:00:00]"></a>[2022年7月08日14:00:00]</h4><p>假使十年前中日韩自贸区没出岔子，三国百姓还会这么辛苦么？</p><p>19年香港动乱时，去查了2012年保钓登岛的渔船，原是居心叵测的跳梁小丑</p><p>对持不同政见者没有偏见，讨厌下作手段</p><p align="right">——安倍晋三遇刺后</p><h4 id="2022年6月30日10-37-00"><a href="#2022年6月30日10-37-00" class="headerlink" title="[2022年6月30日10:37:00]"></a>[2022年6月30日10:37:00]</h4><p>有情绪郁结在心里，舒展不开。有种呕吐的冲动。想一拳打碎眼前的显示器。</p><h4 id="2022年6月14日14-46-00"><a href="#2022年6月14日14-46-00" class="headerlink" title="[2022年6月14日14:46:00]"></a>[2022年6月14日14:46:00]</h4><blockquote><p>当时若爱韩公子，埋骨成灰恨未休。——李商隐《和韩录事送宫人入道》</p></blockquote><p>怀揣过阴暗心思，比同《笑傲江湖》书尾，令狐冲回到华山，在小师妹闺房看到这句诗。</p><p>理性回归，祝福她。“过得比我好，别让我知道”很coooool~~。</p><p>男儿当自强，若干年后念起我，有一缕惆怅，不后悔结识，就够了。</p><h4 id="2022年6月14日18-05-00"><a href="#2022年6月14日18-05-00" class="headerlink" title="[2022年6月14日18:05:00]"></a>[2022年6月14日18:05:00]</h4><p>对女权&#x2F;女拳浪潮持悲观态度：未来女权不会偃旗息鼓。</p><ol><li><p>不损上位者利益，相反可转嫁社会矛盾、引导底层互害。政府妥协损害的是中产男性的利益；</p></li><li><p>有意识组织对抗松散团体。依仗政治正确，夺取话语权，并实际得利，巩固、扩大基本盘。同动物保护、泛清真化别无二致。</p></li></ol><h4 id="2022年6月05日17-01-00"><a href="#2022年6月05日17-01-00" class="headerlink" title="[2022年6月05日17:01:00]"></a>[2022年6月05日17:01:00]</h4><p>十年前离开家乡：我压线考入了985院校，专业在未来炙手可热；喜欢的姑娘，我是她最好的朋友；家庭幸福和睦，给予我无限的理解和支持。<br>可是这十年，我失败至极，锐气消磨殆尽，喜欢的女孩儿渐行渐远了，父母仍要为了我操劳。</p><p>一无所获。</p><blockquote><p>直到最后一刻，都不可以放弃希望。一旦死心的话，比赛就已经结束了。——安西教练《灌篮高手》</p></blockquote><p><img src="/../assets/42eb99d699afa25b0e8c2eff2212c3f2.jpeg"></p><p><em><strong>Best Wishes</strong></em></p><h4 id="2022年5月10日11-11-00"><a href="#2022年5月10日11-11-00" class="headerlink" title="[2022年5月10日11:11:00]"></a>[2022年5月10日11:11:00]</h4><p>孟晚舟、丁真、谷爱凌一个接一个的出现，我只看到宣传口的敷衍和傲慢，神龛脚不着地，庙堂远离江湖。外网丢盔弃甲，内网肆无忌惮。</p><h4 id="2022年3月26日21-46-59"><a href="#2022年3月26日21-46-59" class="headerlink" title="[2022年3月26日21:46:59]"></a>[2022年3月26日21:46:59]</h4><p><em><strong>Old soldiers never die, they simply fade away.</strong></em></p><p><img src="/../assets/001.jpg"></p><h4 id="2022年3月25日17-29-35"><a href="#2022年3月25日17-29-35" class="headerlink" title="[2022年3月25日17:29:35]"></a>[2022年3月25日17:29:35]</h4><p>之前见过一个翻译：“一遇杨过误终身”——<em>Young fault lasts all the life</em>，我也有<em>Young fault</em>，还有弥补的机会！</p><p>逛“服刑家属”贴吧：</p><ol><li>见惯尔虞我诈，这个吧氛围很别致，人间自有真情在</li><li>近年来，国家惩治互联网信息倒卖和隐私保护力度上在加强，有多例因计算机信息类犯罪被判重刑</li><li>好奇鸳鸯团聚后，能温存多久？</li></ol><h4 id="2022年3月21日19-33-02"><a href="#2022年3月21日19-33-02" class="headerlink" title="[2022年3月21日19:33:02]"></a>[2022年3月21日19:33:02]</h4><p><em><strong>I’m Malenia, Blade of Miquella.</strong></em></p><p>已经2022年了，《凛冬的寒风》果然还没出。</p><p>《艾尔登法环》故事的起点是“阴谋之夜”、《冰火》的起点是国王之手的去世，都因某个位高权重人物的离奇去世拉开序幕。古龙、先民、黄金家族，这些设定都有浓浓的冰火味道。</p><p>在Redman Casle就想起詹姆吐槽他姐姐是“自以为长着乳房的泰温公爵”。</p><p>小恶魔的那句<em>I’m guilty of being a dwarf, I’ve been on trial for that my entire life.<em>张力爆表。我有资格说感同身受，但私自把台词改成了</em>I’ve been on trial for that half of my life</em>，鼓舞自己自信起来，早早迈过这道坎。</p><p><img src="/../assets/005.webp"></p><p>古龙的打斗像CS，即使技高一筹，临场发挥和心态也很重要，所以天机老人、上官金虹都输了。金庸的对决像Dota，点好技能买完装备，胜负手已然明了。金庸会解释穴位、动作和角色心理活动，古龙更多是“他来了、他动了、他赢了”。</p><h4 id="2021年12月31日24-00-00"><a href="#2021年12月31日24-00-00" class="headerlink" title="[2021年12月31日24:00:00]"></a>[2021年12月31日24:00:00]</h4><p>要乐观起来啊</p><p><strong>髀肉复生</strong></p><p>Gap这年末期，看了《三国演义》和《邓小平时代》。一帆风顺太难得，多如刘备、李密，奔波半生也不得善终。</p><p>中学时，受易中天的影响，更喜欢曹操和孙权。加上孙彦军表演出色，总觉得刘备假仁假义，最后白帝城郁郁而终实在配不上”英雄”二字。</p><p>李敖推崇邓小平先生，说他三起三落还能挽大厦于将倾。现在想来，玄德公五易其主、四失妻子，尚有“刘郎才气”高志，更是难得。</p><p>相较隋炀帝困据江都时，顾镜“好头颅 谁当斫之”。或是贾谊、李贺这等人杰，年少成名，忧愤而亡。”不夺其志”有多困难，也只有在我低谷时才能体会。</p><p>高处跌落后，消极避世，再不思进取，依靠垃圾信息堵塞感官、填充不平的心，转移注意力，这就是我做的。</p><p><strong>不开新题</strong></p><p>我希望自己明白，真正的生活时刻都在进行着，而不是像开关一样，等到某个时间点才被打开</p><p>不要再问自己“未来还未来”这样的话了</p><p>现在做不到的，不要妄想将来就能做到</p><p>我希望自己在精神方面：慎独、自律、乐观、自信，热爱生命</p><p>我希望自己在物质层面：富裕、健康、拥有一个幸福美满的家庭和事业</p><p>为之奋斗吧，不要忘了享受生活</p>]]></content>
    
    
    
    <tags>
      
      <tag>Life</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0698.深度优先搜索</title>
    <link href="/2024/03/10/0689/"/>
    <url>/2024/03/10/0689/</url>
    
    <content type="html"><![CDATA[<p>A DFS problem from leetcode <a href="https://leetcode.com/problems/partition-to-k-equal-sum-subsets/">698. Partition to K Equal Sum Subsets</a></p><span id="more"></span><p>I find my old version solution in 2018 invalid this try.<br>Then I look up some of most votes items and meet the same situation.<br>I get a dfs solution which is faster than the bit-mask method.<br>In the end, I think this problem should be <em><strong>Hard</strong></em> level because of the rigid time limit.</p><p>We use three trickes to meet the requirment:</p><ol><li>end straight when there is no bucket can hold <code>nums[index]</code></li><li>end straight when <code>index</code> arrive the length of <code>nums</code> without examination of <code>cur_sum</code> array</li><li>reversed sort array <code>nums</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">canPartitionKSubsets</span>(<span class="hljs-params">self, nums, k</span>):<br>        target = <span class="hljs-built_in">sum</span>(nums)<br>        <span class="hljs-keyword">if</span> target % k != <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        target //= k<br>        cur = [<span class="hljs-number">0</span>] * k; nums.sort( reverse = <span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">foo</span>(<span class="hljs-params"> index</span>):<br>            <span class="hljs-keyword">if</span> index == <span class="hljs-built_in">len</span>( nums): <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>( k):<br>                <span class="hljs-keyword">if</span> nums[index] + cur[i] &lt;= target:<br>                    cur[i] += nums[index]<br>                    <span class="hljs-keyword">if</span> foo( index + <span class="hljs-number">1</span>) == <span class="hljs-literal">True</span>: <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>                    cur[i] -= nums[index]<br>                <span class="hljs-keyword">if</span> cur[i] == <span class="hljs-number">0</span>: <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> foo( <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>Thanks for your issue. I’ll list my thinking progress.</p><h2 id="1-For-the-first-tips"><a href="#1-For-the-first-tips" class="headerlink" title="1. For the first tips:"></a>1. For the first tips:</h2><p>Our target is to divide an array <code>nums</code> with size <code>length</code> to <code>k</code> different bucket with same sum.<br>If we could make it, there must be a bucket holding <code>nums[index]</code>, and the bucket’s order doesn’t matter.</p><p>Inner the <code>for i in range(k)</code> loop, before the <code>i</code>-th loop, <code>cur[i]</code> which means the current sum of <code>i-th</code> bucket has two situations:</p><ul><li><code>cur[i] != 0</code></li><li><code>cur[i] == 0</code>: it means the i-th bucket is empty.<br>However, there is an unspoken rules: <code>cur[j] = 0 if j &gt; i and cur[i] == 0</code>. If after this step, the empty bucket <code>i</code> couldn’t hold <code>nums[index]</code>, neither the later.</li></ul><p>It’s an algorithm design paradigm named <strong>Branch and bound</strong></p><h2 id="2-For-the-second-tips"><a href="#2-For-the-second-tips" class="headerlink" title="2. For the second tips:"></a>2. For the second tips:</h2><p>Lines 10 <code>if nums[index] + cur[i] &lt;= target</code> ensure that any bucket’s sum <code>cur[i] &lt;= target</code></p><p>When we get the <code>index == len( nums)</code> for 0-index arrays, it means:</p><ul><li>any number from <code>nums</code> has been in a bucket</li><li><code>cur[0] + cur[1] + ... + cur[k-1] = sum(nums) = k * target</code></li><li><code>0 &lt;= cur[i] &lt;= target, for i = 0, 1, 2, ..., k- 1</code></li></ul><p>If there is a <code>cur[i]</code> not equals to <code>target</code>, <code>sum( cur)</code> must be less than k * target.</p><p>We use <strong>loop invariant</strong> to prove it.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prefix Tuning 介绍</title>
    <link href="/2024/03/10/02_prefix_tuning/"/>
    <url>/2024/03/10/02_prefix_tuning/</url>
    
    <content type="html"><![CDATA[<p>本文旨在结合原文与Peft源码，介绍Prefix Tuning</p><span id="more"></span><ul><li><input checked="" disabled="" type="checkbox"> 可训练参数的位置</li><li><input checked="" disabled="" type="checkbox"> 参数结构</li><li><input checked="" disabled="" type="checkbox"> 前向传播流程</li></ul><p>读懂本文的前提：</p><ul><li>理解NLP、Transformer结构</li><li>熟悉Pytorch&#x2F;Python3</li></ul><p>Prefix Tuning是提供相较于全量微调更为轻量级的选项，主要用于自然语言生成（natural language generation, NLG）任务</p><blockquote><p>In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting.</p></blockquote><h2 id="可训练参数的位置"><a href="#可训练参数的位置" class="headerlink" title="可训练参数的位置"></a>可训练参数的位置</h2><p>如下图，可训练参数，位于transformer激活层的左侧</p><p><img src="/../assets/555.png"></p><p>“Prefix Tuning可以视为Prompt Tuning变种”，此说法不准确，区别有二：</p><ol><li><p>Prefix Tuning插入位置，位于隐藏层，左侧，也就是上图虚线左侧的位置，因此它不会影响原有<code>token length</code>的有效长度。而Prompt tuning可训练参数，插入的位置位于虚线右侧，蓝色输入层左边。它会挤占原有输入层的位置，因此会缩减<code>token length</code>的有效长度。</p></li><li><p>Prefix Tuning会在每一层影藏层都插入可训练参数，它的参数规模要比Prompt Tuning多几倍</p></li></ol><blockquote><p>Meanwhile, this is less expressive than intervening all layers of the activations (§7.2), which avoids long-range dependencies and includes more tunable parameters. Prefix-tuning, therefore, optimizes all layers of the prefix.</p></blockquote><h2 id="参数结构"><a href="#参数结构" class="headerlink" title="参数结构"></a>参数结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># model.py</span><br><span class="hljs-comment"># File Path: https://github.com/huggingface/peft/blob/main/src/peft/tuners/prefix_tuning/model.py</span><br><span class="hljs-comment"># Based on https://github.com/THUDM/P-tuning-v2/blob/main/model/prefix_encoder.py</span><br><span class="hljs-comment"># line 21</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PrefixEncoder</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.prefix_projection = config.prefix_projection<br>        token_dim = config.token_dim<br>        num_layers = config.num_layers<br>        encoder_hidden_size = config.encoder_hidden_size<br>        num_virtual_tokens = config.num_virtual_tokens<br>        <span class="hljs-keyword">if</span> self.prefix_projection <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> config.inference_mode:<br>            <span class="hljs-comment"># Use a two-layer MLP to encode the prefix</span><br>            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)<br>            self.transform = torch.nn.Sequential(<br>                torch.nn.Linear(token_dim, encoder_hidden_size),<br>                torch.nn.Tanh(),<br>                torch.nn.Linear(encoder_hidden_size, num_layers * <span class="hljs-number">2</span> * token_dim),<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * <span class="hljs-number">2</span> * token_dim)    <br>    <br></code></pre></td></tr></table></figure><p>直接看最后一行<code>self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim) </code></p><ul><li><code>num_virtual_tokens</code>：是插入的prefix长度，是上图中的prefix_length</li><li><code>num_layers</code>：是transformer_backbone的层数</li><li><code>token_dim</code>：是token向量化embeded后的维度，transformer中常与<code>encoder_hidden_size</code>相等，即图中的<code>hidden dimension</code></li></ul><p><code>num_layers * 2 * token_dim</code>，参数之所以要$\cdot2$是要同时设置<code>key</code>和<code>value</code></p><blockquote><p>$h_i^{(n)}$is composed of a key-value pair. In GPT-2, the dimension of each key and value is 1024.</p></blockquote><p>上图虚线左侧的部分，可以直观的体现出可训练参数的结构和数量，与代码一致</p><h3 id="prefix-preojection"><a href="#prefix-preojection" class="headerlink" title="prefix_preojection"></a>prefix_preojection</h3><p>关于上述代码条件分支语句<code>if self.prefix_projection and not config.inference_mode:</code>分支的作用：</p><p>作者发现，直接使用单层结构进行训练，无法稳定收敛，对学习率和初始化方式敏感。因此额外引入全连接层，将prefix映射至其它线性空间。在训练结束后，映射前的prefix向量和全连接层即可丢弃，只保留prefix在新线性空间的投影，作为前向传播时前置隐藏层。</p><blockquote><p>Empirically, directly updating the $P_{\theta}$ parameters leads to unstable optimization and a slight drop in performance. So we reparametrize the matrix $P_{\theta}[i,:]&#x3D;MLP_{\theta}(P^{‘}<em>{\theta}[i,:])$ by a smaller matrix $P^{‘}</em>{\theta}$ composed with a large feedforward neural network ($MLP_{\theta}$). Note that $P_{\theta}$ and $P^{‘}<em>{\theta}$  has the same rows dimension (i.e. the prefix length), but different columns dimension. Once training is complete, these reparametrization parameters can be dropped, and only the prefix ($P</em>{\theta}$​) needs to be saved.</p></blockquote><blockquote><p>We find in preliminary experiments that directly optimizing the prefix is very sensitive to the learning rate and initialization.</p></blockquote><blockquote><p>$P_{\theta}$ has a dimension of  $|P_{idx}\times dim(h_i)|$ while $P_\theta$ has a dimension of $|P_{idx}\times k|$, where we choose $k&#x3D;512$ for table-to-text and 800 for summarization. $MLP_{\theta}$ maps from dimension $k$ to $dim_({h_i})$ </p></blockquote><p><code>prefix_preojection</code>参数在peft框架中是可选项，选择是否映射。</p><h3 id="前向传播流程"><a href="#前向传播流程" class="headerlink" title="前向传播流程"></a>前向传播流程</h3><p>在引入Prefix之前，模型的前向传播公式：</p><p>$$<br>h_i&#x3D;LM_{\Theta}(z_i,h_{&lt;i})<br>$$</p><p>在引入Prefix之后（参照下图），模型的前向传播公式，其中$P_{\theta}$是新增的可训练参数矩阵，$P_{\theta}\in \mathbb{R}^{ |P_{idx}| \times dim(h_i)}$：<br>$$<br>h_i&#x3D;<br>\begin{cases}<br> P_{\theta}[i, :] &amp; \text{if } i \in P_{\text{idx}} \<br> LM_{\phi}(z_{i}, h_{&lt;i}) &amp; \text{otherwise}<br>\end{cases}<br>$$<br>当  $i \in P_{idx}$ 时，$h_i$的结果当然依赖可训练参数$\theta$；当 $i \notin P_{idx}$时，此时$h_i$的结果依赖所有之前的$h_{&lt;i}$，所以同样受到$\theta$的影响。</p><blockquote><p>Here, $h_i$ (for all $i$) is a function of the trainable $P_{\theta}$. When $i \in P_{idx}$, this is clear because $h_i$ copies directly from $P_{\theta}$. When $i \notin P_{idx}$, $h_i$ still depends on $P_{\theta}$, because the prefix activations are always in the left context and will therefore affect any activations to its right.</p></blockquote><p>下图上方是autogressive LM，图下方为encoder-decoer model。Prefix激活层$\forall i \in P_{idx}$, $h_i$ 从可训练参数$P_{\theta}$中获得，余下的激活层（隐藏层）由正常的Transformer流程产生。</p><blockquote><p>An annotated example of prefix-tuning using an autoregressive LM (top) and an encoder-decoder model (bottom). The prefix activations $\forall i \in P_{idx}$, $h_i$ are drawn from a trainable matrix $P_{\theta}$. The remaining activations are computed by the Transformer.</p></blockquote><p><img src="/../assets/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE(8).png"></p><p><em>talk is cheap, show me your code.</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># peft_model.py</span><br><span class="hljs-comment"># https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py</span><br><span class="hljs-comment"># line 1116</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PeftModelForCausalLM</span>(<span class="hljs-title class_ inherited__">PeftModel</span>):<br>    <span class="hljs-comment"># example file is peft_prefix_tuning_clm.ipynb</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_ids=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        inputs_embeds=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        labels=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        output_attentions=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        output_hidden_states=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        return_dict=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        task_ids=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        **kwargs,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> peft_config.peft_type == PeftType.PREFIX_TUNING:<br>            past_key_values = self.get_prompt(batch_size)<br>            <span class="hljs-keyword">return</span> self.base_model(<br>                input_ids=input_ids, inputs_embeds=inputs_embeds, past_key_values=past_key_values, **kwargs<br>            )<br></code></pre></td></tr></table></figure><p>看向最后一行，Prefix Tuning在执行前向传播的流程时，根据Prefix长度，提前设置好input_ids和past_key_values，就可以借助现有模型实现前向传播模型。通过挂载的方式实现，而无需修改任何原有模型结构。</p><ul><li><code>input_ids</code>是由在prepare_process阶段，提前准备好的索引index和词向量，可以参考<code>peft_prefix_tuning_clm.ipynb</code></li><li><code>inputs_embeds is None</code></li><li><code>kwargs</code>还包含<code>attention_mask</code>和<code>labels</code></li><li><code>past_key_values</code>的准备过程如下</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># peft_model.py</span><br><span class="hljs-comment"># https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py</span><br><span class="hljs-comment"># line 447   </span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_prompt</span>(<span class="hljs-params">self, batch_size: <span class="hljs-built_in">int</span>, task_ids: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span></span>) -&gt; torch.Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Returns the virtual prompts to use for Peft. Only applicable when using a prompt learning method.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        peft_config = self.active_peft_config<br>        prompt_encoder = self.prompt_encoder[self.active_adapter]<br>        prompt_tokens = (<br>            self.prompt_tokens[self.active_adapter]<br>            .unsqueeze(<span class="hljs-number">0</span>)<br>            .expand(batch_size, -<span class="hljs-number">1</span>)<br>            .to(prompt_encoder.embedding.weight.device)<br>        )<br>        <span class="hljs-comment"># ... ....</span><br>        <span class="hljs-keyword">return</span> past_key_values<br>        <br></code></pre></td></tr></table></figure><ul><li>代码中的<code>prompt_encode</code>就是<strong>参数结构</strong>代码中准备的<code>self.embedding</code></li><li><code>.expand( batch_size, -1)</code>，即是为batch中的每一组输入配置相同的<code>prefix</code></li></ul><p>如何将<code>self.embedding</code>一个$num_virtual_tokens \times num_layers * 2 * token_dim$的大矩阵拆解不在此展开</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a></p><p><a href="https://github.com/huggingface/peft">huggingface&#x2F;peft</a></p><p><a href="https://huggingface.co/docs/transformers/model_doc/bloom">Transformer-based Bloom</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>模型微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt Tuning</title>
    <link href="/2024/03/10/01_prompt_tuning/"/>
    <url>/2024/03/10/01_prompt_tuning/</url>
    
    <content type="html"><![CDATA[<p>本文旨在结合原文与Peft源码，介绍Prompt Tuning</p><span id="more"></span><ul><li><input checked="" disabled="" type="checkbox"> 可训练参数的位置</li><li><input checked="" disabled="" type="checkbox"> 参数结构</li><li><input checked="" disabled="" type="checkbox"> 前向传播流程</li></ul><h2 id="可训练参数的位置"><a href="#可训练参数的位置" class="headerlink" title="可训练参数的位置"></a>可训练参数的位置</h2><p>Prompt Tuning只引入一层参数，位于Transformer的输入位置（图中绿色部分）</p><p><img src="/../assets/one.png"></p><h2 id="参数结构"><a href="#参数结构" class="headerlink" title="参数结构"></a>参数结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># peft_model.py </span><br><span class="hljs-comment"># line 389</span><br><span class="hljs-comment"># named_param = &#x27;word_embeddings.weight&#x27;</span><br>self.word_embeddings = transformer_backbone.get_submodule(named_param.replace(<span class="hljs-string">&quot;.weight&quot;</span>, <span class="hljs-string">&quot;&quot;</span>))<br><br><br><span class="hljs-comment"># peft_model.py </span><br><span class="hljs-comment"># line 393</span><br><span class="hljs-comment"># config是PromptTuning的参数</span><br><span class="hljs-comment"># self.word_embedding是原模型最底层Transformer的word_embedding</span><br>prompt_encoder = PromptEmbedding(config, self.word_embeddings)<br></code></pre></td></tr></table></figure><p>用于推理的参数是prompt_encoder对象，可训练的实数参数的数量为</p><p>$$<br>T &#x3D; E\cdot P<br>$$</p><ul><li>$T$：可训练参数的规模</li><li>$E$ ：<code>token embedding</code>的维度</li><li>$P$​：预设的prompt长度</li></ul><blockquote><p>The parameter cost of our method is EP, where E is the token embedding dimension and P is the prompt length.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PromptEmbedding</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, word_embeddings</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules<br>        self.embedding = torch.nn.Embedding(total_virtual_tokens, config.token_dim)<br>        <span class="hljs-keyword">if</span> config.prompt_tuning_init == PromptTuningInit.TEXT <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> config.inference_mode:<br>            <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br>            tokenizer_kwargs = config.tokenizer_kwargs <span class="hljs-keyword">or</span> &#123;&#125;<br>            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path, **tokenizer_kwargs)<br>            init_text = config.prompt_tuning_init_text<br>            init_token_ids = tokenizer(init_text)[<span class="hljs-string">&quot;input_ids&quot;</span>]<br>            num_text_tokens = <span class="hljs-built_in">len</span>(init_token_ids)<br>            <span class="hljs-keyword">if</span> num_text_tokens &gt; total_virtual_tokens:<br>                init_token_ids = init_token_ids[:total_virtual_tokens]<br>            <span class="hljs-keyword">elif</span> num_text_tokens &lt; total_virtual_tokens:<br>                num_reps = math.ceil(total_virtual_tokens / num_text_tokens)<br>                init_token_ids = init_token_ids * num_reps<br>            init_token_ids = init_token_ids[:total_virtual_tokens]<br>            init_token_ids = torch.LongTensor(init_token_ids).to(word_embeddings.weight.device)<br><br>            word_embedding_weights = word_embeddings(init_token_ids).detach().clone()<br>            word_embedding_weights = word_embedding_weights.to(torch.float32)<br>            self.embedding.weight = torch.nn.Parameter(word_embedding_weights)<br></code></pre></td></tr></table></figure><p><code>total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules</code>代表允许prompt tuning插入的tokens长度</p><p><code>config.prompt_tuning_init_text</code>是初始化的prompt文本</p><p>整段代码的工作逻辑是</p><ol><li>传入预设的prompt tuning参数以及原大模型底层transformer的embedding层</li><li>获取原大模型使用的tokenizer</li><li>利用tokenizer将prompt文本转化成token ids，并截取或扩充至<code>total_virtual_tokens</code>长度</li><li>使用resize后的token ids从embeding选择初始化参数（peft的选择）</li></ol><p>以下是prompt tuning关于初始化的原文<strong>（2.1  Design Decisions）</strong>：</p><blockquote><p>A more sophisticated option is to initialize each prompt token to an embedding drawn from the model’s vocabulary.</p><p>Since we want the model to produce these tokens in the output, initializing the prompt with the embeddings of the valid target tokens should prime the model to restrict its output to the legal output classes.</p></blockquote><h2 id="前向传播流程"><a href="#前向传播流程" class="headerlink" title="前向传播流程"></a>前向传播流程</h2><p>假如word vector的长度是1024，引入Prompt Tuning之前，$input.shape&#x3D;8\times8000\times1024$，引入长度为8的Promt Tuning，得到参数维度 $prompt_encoder.embedding.weight.shape &#x3D; 8 \times 8\times 1024$，拼接成新输入$\hat{input}.shape&#x3D;8 \times 8008\times1024$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># peft_model.py</span><br><span class="hljs-comment"># line 1123~1130</span><br>inputs_embeds = self.word_embeddings( input_ids) <span class="hljs-comment"># 原始输入</span><br>prompts = prompt_encoder.embedding.weight.repeat( batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># prompt tuning 参数</span><br>inputs_embeds = torch.cat( ( prompts, inputs_embeds), dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># 组合输入</span><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/huggingface/peft">huggingface&#x2F;peft</a></p><p><a href="https://arxiv.org/abs/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>模型微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0152.一道非典型动态规划题</title>
    <link href="/2024/03/10/0152/"/>
    <url>/2024/03/10/0152/</url>
    
    <content type="html"><![CDATA[<p>Leetcode动态规划的题目<a href="https://leetcode.com/problems/maximum-product-subarray/">152. Maximum Product Subarray</a></p><span id="more"></span><p>题目不难，是<a href="https://leetcode.com/problems/maximum-subarray/">53. Maximum Subarray</a>的拓展。</p><p>标准的解法就是动态规划，时间复杂度O(n)，空间复杂度O(1)。《算法导论》为了介绍分治，在第四章Divide-and-Conquer，介绍了另一种方法，时间复杂度为O(nlogn)，此处不再赘述。</p><p><a href="https://leetcode.com/problems/maximum-product-subarray/discuss/183483/JavaC%2B%2BPython-it-can-be-more-simple">lee215</a>大牛的另一种解法令人耳目一新:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">maxProduct</span>(<span class="hljs-params">self, A</span>):<br>    B = A[::-<span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(A)):<br>        A[i] *= A[i - <span class="hljs-number">1</span>] <span class="hljs-keyword">or</span> <span class="hljs-number">1</span><br>        B[i] *= B[i - <span class="hljs-number">1</span>] <span class="hljs-keyword">or</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(A + B)<br></code></pre></td></tr></table></figure><p>首先，用0把数组划分为若干个子数组，最终的结果在0和不含0子数组中产生。</p><p><img src="/../assets/001.png" alt="001"></p><p>以一个不含0的长度为n的<strong>整数数组</strong>nums为例，最终可以证明，<strong>该数组的最大乘积，一定是从某端点出发，延展到某个位置为止</strong>。</p><p><img src="/../assets/002.png" alt="002"></p><p>分类讨论</p><ol><li>假如这个数组没有负数，最大乘积一定是从最左端到最右端，结论成立</li><li>假设这个数组包含1个负数，负数位于index，那最大乘积一定是由<code>nums[0:index+1]</code>或者<code>nums[index:n]</code>产生，结论成立</li></ol><p><img src="/../assets/003.png" alt="003"></p><ol><li>假如这个数组包含偶数个负数，最大乘积一定是从最左端到最右端，结论成立</li><li>假如这个数组包含3个负数，分割点一定是第一个负数(i)或者第三个负数(k)，结论成立</li></ol><p>依此类推，这个结论是成立的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
